# -*- coding: utf-8 -*-
"""
Notebook 1: Nettoyage et Pr√©paration des Donn√©es
Projet BI - Analyse des Coliques N√©phr√©tiques
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# Configuration des graphiques
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*80)
print("√âTAPE 1: CHARGEMENT DES DONN√âES")
print("="*80)

# Charger les donn√©es
df = pd.read_csv('data9.csv')

print(f"\n‚úì Donn√©es charg√©es avec succ√®s!")
print(f"  - Nombre de lignes: {df.shape[0]}")
print(f"  - Nombre de colonnes: {df.shape[1]}")
print(f"\nAper√ßu des premi√®res lignes:")
print(df.head())

# Informations sur les donn√©es
print("\n" + "="*80)
print("INFORMATIONS SUR LES DONN√âES")
print("="*80)
print(df.info())

# Statistiques descriptives
print("\n" + "="*80)
print("STATISTIQUES DESCRIPTIVES")
print("="*80)
print(df.describe())

print("\n" + "="*80)
print("√âTAPE 2: ANALYSE DES VALEURS MANQUANTES")
print("="*80)

# Analyse des valeurs manquantes
missing_values = df.isnull().sum()
missing_percentage = (df.isnull().sum() / len(df)) * 100
missing_df = pd.DataFrame({
    'Colonne': missing_values.index,
    'Valeurs_Manquantes': missing_values.values,
    'Pourcentage': missing_percentage.values
})
missing_df = missing_df[missing_df['Valeurs_Manquantes'] > 0].sort_values('Valeurs_Manquantes', ascending=False)

print(f"\n‚úì Colonnes avec valeurs manquantes: {len(missing_df)}")
if len(missing_df) > 0:
    print("\n" + missing_df.to_string(index=False))
    
    # Visualisation
    plt.figure(figsize=(12, 6))
    plt.bar(range(len(missing_df)), missing_df['Pourcentage'])
    plt.xticks(range(len(missing_df)), missing_df['Colonne'], rotation=90)
    plt.ylabel('Pourcentage de valeurs manquantes (%)')
    plt.title('Valeurs manquantes par colonne')
    plt.tight_layout()
    plt.savefig('01_valeurs_manquantes.png', dpi=300, bbox_inches='tight')
    print("\n‚úì Graphique sauvegard√©: 01_valeurs_manquantes.png")
    plt.close()

print("\n" + "="*80)
print("√âTAPE 3: D√âTECTION DES DOUBLONS")
print("="*80)

# V√©rifier les doublons
duplicates = df.duplicated().sum()
print(f"\n‚úì Nombre de doublons d√©tect√©s: {duplicates}")

if duplicates > 0:
    print("  Suppression des doublons...")
    df = df.drop_duplicates(keep='first')
    print(f"  ‚úì Doublons supprim√©s. Nouvelles dimensions: {df.shape}")

print("\n" + "="*80)
print("√âTAPE 4: GESTION DES VALEURS ABERRANTES")
print("="*80)

# Fonction pour d√©tecter les valeurs aberrantes
def detect_outliers(data, column, lower_bound, upper_bound):
    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
    return len(outliers), outliers

# V√©rifier les plages cliniquement plausibles
print("\nüîç V√©rification des plages cliniquement plausibles:")

# Age
age_outliers, _ = detect_outliers(df, 'Age', 0, 120)
print(f"  - Age (0-120 ans): {age_outliers} valeurs aberrantes")

# Temp√©rature
temp_outliers, _ = detect_outliers(df, 'Temp√©rature', 34, 42)
print(f"  - Temp√©rature (34-42¬∞C): {temp_outliers} valeurs aberrantes")

# Fr√©quence cardiaque
fc_outliers, _ = detect_outliers(df, 'FC', 30, 220)
print(f"  - FC (30-220 bpm): {fc_outliers} valeurs aberrantes")

# Pression art√©rielle systolique
pas_outliers, _ = detect_outliers(df, 'PAS', 60, 250)
print(f"  - PAS (60-250 mmHg): {pas_outliers} valeurs aberrantes")

# SPO2
spo2_outliers, _ = detect_outliers(df, 'SPO2', 70, 100)
print(f"  - SPO2 (70-100%): {spo2_outliers} valeurs aberrantes")

# Filtrage des valeurs aberrantes
print("\nüìä Filtrage des valeurs aberrantes...")
df_cleaned = df[
    (df['Age'] >= 0) & (df['Age'] <= 120) &
    (df['Temp√©rature'] >= 34) & (df['Temp√©rature'] <= 42) &
    (df['FC'] >= 30) & (df['FC'] <= 220) &
    (df['PAS'] >= 60) & (df['PAS'] <= 250) &
    (df['SPO2'] >= 70) & (df['SPO2'] <= 100)
].copy()

print(f"  ‚úì Lignes conserv√©es: {len(df_cleaned)}/{len(df)} ({len(df_cleaned)/len(df)*100:.1f}%)")

print("\n" + "="*80)
print("√âTAPE 5: IMPUTATION DES VALEURS MANQUANTES")
print("="*80)

# Imputation logique pour ATCDS
print("\nüîß Imputation logique pour ATCDS:")
# Colonnes d'ant√©c√©dents sp√©cifiques
antecedent_cols = ['Diab√®te', 'HTA', 'Dyslipid√©mie', 'FA', 'Insuffisance_r√©nale']

# Compter les valeurs manquantes avant
atcds_missing_before = df_cleaned['ATCDS'].isnull().sum()

# Imputation bas√©e sur les ant√©c√©dents sp√©cifiques
for idx, row in df_cleaned[df_cleaned['ATCDS'].isnull()].iterrows():
    has_antecedent = False
    for col in antecedent_cols:
        if pd.notna(row[col]) and row[col] == 2.0:  # 2 = Oui
            has_antecedent = True
            break
    
    if has_antecedent:
        df_cleaned.loc[idx, 'ATCDS'] = 2.0  # A des ant√©c√©dents
    else:
        df_cleaned.loc[idx, 'ATCDS'] = 4.0  # Pas d'ant√©c√©dents

atcds_missing_after = df_cleaned['ATCDS'].isnull().sum()
print(f"  ‚úì ATCDS imput√©: {atcds_missing_before - atcds_missing_after} valeurs")

# Imputation bas√©e sur le genre pour Femme_enceinte
print("\nüîß Imputation pour Femme_enceinte:")
femme_enceinte_before = df_cleaned['Femme_enceinte'].isnull().sum()

# Pour les hommes (Genre=2), mettre 4 (Non)
df_cleaned.loc[df_cleaned['Genre'] == 2.0, 'Femme_enceinte'] = df_cleaned.loc[
    df_cleaned['Genre'] == 2.0, 'Femme_enceinte'
].fillna(4.0)

# Pour les femmes (Genre=4), utiliser le mode
mode_femme = df_cleaned[df_cleaned['Genre'] == 4.0]['Femme_enceinte'].mode()[0]
df_cleaned.loc[df_cleaned['Genre'] == 4.0, 'Femme_enceinte'] = df_cleaned.loc[
    df_cleaned['Genre'] == 4.0, 'Femme_enceinte'
].fillna(mode_femme)

femme_enceinte_after = df_cleaned['Femme_enceinte'].isnull().sum()
print(f"  ‚úì Femme_enceinte imput√©: {femme_enceinte_before - femme_enceinte_after} valeurs")

# Imputation par mode pour les autres colonnes avec peu de valeurs manquantes
print("\nüîß Imputation par mode pour les colonnes avec peu de valeurs manquantes:")
columns_to_impute = ['febrile', 'AUSP']

for col in columns_to_impute:
    if col in df_cleaned.columns and df_cleaned[col].isnull().sum() > 0:
        missing_before = df_cleaned[col].isnull().sum()
        mode_val = df_cleaned[col].mode()[0]
        df_cleaned[col].fillna(mode_val, inplace=True)
        print(f"  ‚úì {col}: {missing_before} valeurs imput√©es avec mode={mode_val}")

# Supprimer la colonne Longueur_DPC si elle a trop de valeurs manquantes
if 'Longueur_DPC' in df_cleaned.columns:
    missing_pct = df_cleaned['Longueur_DPC'].isnull().sum() / len(df_cleaned) * 100
    if missing_pct > 50:
        print(f"\n‚ö†Ô∏è  Suppression de Longueur_DPC ({missing_pct:.1f}% manquant)")
        df_cleaned = df_cleaned.drop('Longueur_DPC', axis=1)

print("\n" + "="*80)
print("√âTAPE 6: BINARISATION DES VARIABLES")
print("="*80)

# Fonction pour binariser (2,4) -> (1,0)
def binarize_column(col_data):
    """Convertir (2,4) en (1,0) ou (4,6) en (0,1)"""
    unique_vals = col_data.dropna().unique()
    
    if set(unique_vals).issubset({2.0, 4.0}):
        return col_data.map({2.0: 1, 4.0: 0})
    elif set(unique_vals).issubset({4.0, 6.0}):
        return col_data.map({4.0: 0, 6.0: 1})
    else:
        return col_data

# Identifier et binariser les colonnes binaires
print("\nüîß Binarisation des variables cod√©es (2,4) ou (4,6):")
binary_count = 0

for col in df_cleaned.columns:
    if df_cleaned[col].dtype in ['float64', 'int64']:
        unique_vals = df_cleaned[col].dropna().unique()
        if len(unique_vals) == 2:
            if set(unique_vals).issubset({2.0, 4.0}) or set(unique_vals).issubset({4.0, 6.0}):
                df_cleaned[col] = binarize_column(df_cleaned[col])
                binary_count += 1

print(f"  ‚úì {binary_count} colonnes binaris√©es")

print("\n" + "="*80)
print("√âTAPE 7: CR√âATION DE CAT√âGORIES D'√ÇGE")
print("="*80)

# Cr√©er des cat√©gories d'√¢ge
bins = [0, 18, 30, 50, 70, 120]
labels = ['Enfant_Ado', 'Jeune_adulte', 'Adulte', 'Senior', 'Personne_√¢g√©e']
df_cleaned['Categorie_Age'] = pd.cut(df_cleaned['Age'], bins=bins, labels=labels, right=False)

print("\n‚úì Cat√©gories d'√¢ge cr√©√©es:")
print(df_cleaned['Categorie_Age'].value_counts().sort_index())

# Visualisation
plt.figure(figsize=(10, 6))
df_cleaned['Categorie_Age'].value_counts().sort_index().plot(kind='bar', color='steelblue')
plt.title('Distribution des patients par cat√©gorie d\'√¢ge')
plt.xlabel('Cat√©gorie d\'√¢ge')
plt.ylabel('Nombre de patients')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('02_distribution_age.png', dpi=300, bbox_inches='tight')
print("‚úì Graphique sauvegard√©: 02_distribution_age.png")
plt.close()

print("\n" + "="*80)
print("√âTAPE 8: ENGINEERING DE FEATURES - √âVOLUTION DE LA DOULEUR")
print("="*80)

# Calculer la variation de douleur
pain_cols = ['EN0', 'EN_15min', 'EN_30min', 'EN_45min', 'EN_60min', 'EN_90min']

# V√©rifier que toutes les colonnes existent
available_pain_cols = [col for col in pain_cols if col in df_cleaned.columns]
print(f"\n‚úì Colonnes de douleur disponibles: {len(available_pain_cols)}/{len(pain_cols)}")

if 'EN0' in df_cleaned.columns and 'EN_90min' in df_cleaned.columns:
    # Variation de douleur
    df_cleaned['Variation_douleur'] = df_cleaned['EN_90min'] - df_cleaned['EN0']
    
    # Pourcentage de r√©duction de douleur
    df_cleaned['Pct_reduction_douleur'] = (
        (df_cleaned['EN0'] - df_cleaned['EN_90min']) / (df_cleaned['EN0'] + 0.001) * 100
    )
    
    print("\n‚úì Nouvelles features cr√©√©es:")
    print(f"  - Variation_douleur: {df_cleaned['Variation_douleur'].describe()}")
    print(f"  - Pct_reduction_douleur: {df_cleaned['Pct_reduction_douleur'].describe()}")
    
    # Visualisation de l'√©volution de la douleur
    pain_data = df_cleaned[available_pain_cols].mean()
    
    plt.figure(figsize=(12, 6))
    plt.plot(range(len(pain_data)), pain_data, marker='o', linewidth=2, markersize=8)
    plt.title('√âvolution moyenne de la douleur dans le temps')
    plt.xlabel('Mesure de douleur')
    plt.ylabel('Score de douleur moyen')
    plt.xticks(range(len(pain_data)), [col.replace('EN_', '').replace('EN0', '0min') for col in available_pain_cols], rotation=45)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig('03_evolution_douleur.png', dpi=300, bbox_inches='tight')
    print("‚úì Graphique sauvegard√©: 03_evolution_douleur.png")
    plt.close()

print("\n" + "="*80)
print("√âTAPE 9: NORMALISATION DES DONN√âES")
print("="*80)

# Identifier les colonnes num√©riques √† normaliser
from sklearn.preprocessing import MinMaxScaler

# Colonnes √† exclure de la normalisation
exclude_cols = ['Age', 'PAS', 'PAD', 'FC', 'Temp√©rature'] + pain_cols + ['Variation_douleur', 'Pct_reduction_douleur']

# Colonnes num√©riques √† normaliser
numeric_cols = df_cleaned.select_dtypes(include=['float64', 'int64']).columns
cols_to_normalize = [col for col in numeric_cols if col not in exclude_cols and df_cleaned[col].nunique() > 2]

if len(cols_to_normalize) > 0:
    print(f"\n‚úì Normalisation de {len(cols_to_normalize)} colonnes num√©riques")
    scaler = MinMaxScaler()
    df_cleaned[cols_to_normalize] = scaler.fit_transform(df_cleaned[cols_to_normalize])
    print("  ‚úì Normalisation termin√©e (√©chelle 0-1)")
else:
    print("\n  ‚ÑπÔ∏è  Pas de colonnes √† normaliser")

print("\n" + "="*80)
print("√âTAPE 10: R√âSUM√â FINAL ET EXPORT")
print("="*80)

print(f"\nüìä R√âSUM√â DU NETTOYAGE:")
print(f"  - Lignes initiales: {len(df)}")
print(f"  - Lignes finales: {len(df_cleaned)}")
print(f"  - Lignes supprim√©es: {len(df) - len(df_cleaned)} ({(len(df) - len(df_cleaned))/len(df)*100:.1f}%)")
print(f"  - Colonnes initiales: {len(df.columns)}")
print(f"  - Colonnes finales: {len(df_cleaned.columns)}")

# V√©rification finale des valeurs manquantes
final_missing = df_cleaned.isnull().sum().sum()
print(f"\n‚úì Valeurs manquantes restantes: {final_missing}")

# Sauvegarder les donn√©es nettoy√©es
output_file = 'donnees_nettoyees.csv'
df_cleaned.to_csv(output_file, index=False, encoding='utf-8')
print(f"\n‚úÖ Donn√©es nettoy√©es sauvegard√©es: {output_file}")

# Cr√©er un rapport de nettoyage
rapport = f"""
RAPPORT DE NETTOYAGE DES DONN√âES
================================
Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

1. DONN√âES INITIALES
   - Nombre de lignes: {len(df)}
   - Nombre de colonnes: {len(df.columns)}
   - Taille m√©moire: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB

2. NETTOYAGE EFFECTU√â
   - Doublons supprim√©s: {duplicates}
   - Valeurs aberrantes filtr√©es: {len(df) - len(df_cleaned)}
   - Colonnes supprim√©es: {len(df.columns) - len(df_cleaned.columns)}
   - Variables binaris√©es: {binary_count}

3. DONN√âES FINALES
   - Nombre de lignes: {len(df_cleaned)}
   - Nombre de colonnes: {len(df_cleaned.columns)}
   - Taux de conservation: {len(df_cleaned)/len(df)*100:.1f}%
   - Valeurs manquantes: {final_missing}

4. NOUVELLES FEATURES
   - Categorie_Age
   - Variation_douleur
   - Pct_reduction_douleur

5. FICHIERS G√âN√âR√âS
   - donnees_nettoyees.csv
   - 01_valeurs_manquantes.png
   - 02_distribution_age.png
   - 03_evolution_douleur.png
   - rapport_nettoyage.txt
"""

with open('rapport_nettoyage.txt', 'w', encoding='utf-8') as f:
    f.write(rapport)

print(rapport)
print("\n‚úÖ Rapport de nettoyage sauvegard√©: rapport_nettoyage.txt")
print("\n" + "="*80)
print("üéâ NETTOYAGE TERMIN√â AVEC SUCC√àS!")
print("="*80)
print("\nFichiers g√©n√©r√©s:")
print("  1. donnees_nettoyees.csv - Donn√©es pr√™tes pour le Data Warehouse")
print("  2. rapport_nettoyage.txt - Rapport d√©taill√©")
print("  3. 01_valeurs_manquantes.png - Graphique des valeurs manquantes")
print("  4. 02_distribution_age.png - Distribution par √¢ge")
print("  5. 03_evolution_douleur.png - √âvolution de la douleur")
print("\n‚û°Ô∏è  Prochaine √©tape: Cr√©er le Data Warehouse avec SQLite3")
